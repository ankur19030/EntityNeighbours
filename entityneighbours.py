# -*- coding: utf-8 -*-
"""EntityNeighbours.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10aj_RbSu4fTNacjuy2D865rYfzuUxfcJ
"""



from qwikidata.json_dump import WikidataJsonDump
from qwikidata.linked_data_interface import get_entity_dict_from_api
from qwikidata.entity import WikidataItem, WikidataProperty
import lmdb
from contextlib import closing
import six
from uuid import uuid1
from multiprocessing.pool import Pool
import sys

from functools import partial
import zlib
import pickle
from tqdm import tqdm
import multiprocessing as mp


chunk_file = open("WikidataEntityID/"+str(sys.argv[1]),"rb")
chunk_dict= pickle.load(chunk_file)
chunk_file.close()


# Remove the keys which has alreadt been processed from the dict
# Returns the length of the new dict

def preprocess_chunk_dict(out_file):

    global chunk_dict
    new_list = []

    entityNeighboursDB = EntityNeighboursDB(out_file)
    for key in chunk_dict:
        if not entityNeighboursDB.check_key(key):
            new_list.append(key)            
    chunk_dict = new_list
    return len(new_list)



import sys
from SPARQLWrapper import SPARQLWrapper, JSON

endpoint_url = "https://query.wikidata.org/sparql"
entities_failed_list = []



def get_results(endpoint_url, query):
    user_agent = "WDQS-example Python/%s.%s" % (sys.version_info[0], sys.version_info[1])
    # TODO adjust user agent; see https://w.wiki/CX6
    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    return sparql.query().convert()


def preprocess_multiple_results(results):


    result_dict = {}

    for result in results:

        if 'http://www.wikidata.org/prop/direct/' in result['p']['value'] and 'http://www.wikidata.org/entity/' in \
                result['v']['value']:
            entity = result['researcher']['value']
            if entity not in result_dict:
                result_dict[entity] = {}
            property = result['p']['value'].split("/")[-1]
            value = result['v']['value'].split("/")[-1]
            result_dict[entity].setdefault(property, []).append(value)


    all_results = []

    for key, value in result_dict.items():
      all_results.append((key.encode("utf-8"),zlib.compress(pickle.dumps(value))))

    return all_results

def process_multiple_entities(entities):

    all_entities = "\nwd:".join(entities)
    all_entities = "\nwd:" + all_entities

    query = """SELECT ?researcher ?researcherLabel ?p ?v ?vLabel WHERE {
          VALUES ?researcher {
            wd:""" + all_entities + """
          } 
          ?researcher ?p ?v.
          ?x wikibase:directClaim ?p.
          FILTER(CONTAINS(STR(?v), "http://www.wikidata.org/entity/"))
          SERVICE wikibase:label { bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en". }
        }"""

    results = get_results(endpoint_url, query)

    return preprocess_multiple_results(results["results"]["bindings"])


def chunks(lst, n):
    """Yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


def process_all_entities(out_file, pool_size, chunk_size,
          init_map_size=1000000000000, buffer_size=3000,entity_batch_size=1):
  
    total_lines = preprocess_chunk_dict(out_file)/entity_batch_size
    dump_reader = chunks(chunk_dict,entity_batch_size)

    with closing(lmdb.open(out_file, subdir=False, map_async=True, map_size=init_map_size,
                           max_dbs=3)) as env:
        map_size = [init_map_size]
        meta_db = env.open_db(b'__meta__')
        with env.begin(db=meta_db, write=True) as txn:
            txn.put(b'id', six.text_type(uuid1().hex).encode('utf-8'))

            txn.put(b'version', six.text_type("").encode('utf-8')
            )

        entity_db = env.open_db(b'__entity_neighbours__')

        def write_db(db, data):
            try:
                with env.begin(db=db, write=True) as txn:
                    txn.cursor().putmulti(data)

            except lmdb.MapFullError:
                map_size[0] *= 2
                env.set_mapsize(map_size[0])

                write_db(db, data)    

        entity_buf = [] # Writing into the buffer first, to have multiple transaction written to the database simultaneously.

        with tqdm(total=total_lines, mininterval=0.5) as bar:
            
            for entities in dump_reader:
                
                results = process_multiple_entities(entities)

                for result in  results:
                    entity_buf.append(result)

                if len(entity_buf) >= buffer_size:
                    write_db(entity_db, entity_buf)
                    entity_buf = []

                bar.update(1)

            if entity_buf:
                write_db(entity_db, entity_buf)

    
class EntityNeighboursDB:

    def __init__(self,db_file):

        self._env = lmdb.open(db_file, readonly=True, subdir=False, lock=False, max_dbs=3)
        self._entity_db = self._env.open_db(b'__entity_neighbours__')



    def check_key(self,key):

        with self._env.begin(db=self._entity_db) as txn:
            value = txn.get(key.encode('utf-8'))
            if not value:
                return False
            else:
                return True


process_all_entities("ENTITY_NEIGHBOURS_DB"+str(sys.argv[1]),mp.cpu_count(),1)

entities_failed_list_file = open("entities_failed_list"+str(sys.argv[1]),"wb")
pickle.dump(entities_failed_list,entities_failed_list_file)
entities_failed_list_file.close()

